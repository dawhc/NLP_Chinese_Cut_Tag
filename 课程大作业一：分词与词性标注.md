### 课程大作业一：分词与词性标注

##### 1120182424 崔冬航

---

1. 实现目标

   * 实现基于词典的中文分词方法和基于统计模型的中文分词方法
   * 对分词结果进行词性标注
   * 对分词及词性标注进行测试，对结果进行四个指标的评价：精确率Precision、召回率Recall、F1值以及运行效率

2. 整体程序结构

   2.1 分词（separator.py）

   > 将分词功能封装成分词器类Separator，其中包含了基于词典的分词方法和基于统计的分词方法。其包含的主要方法如下：
   >
   > ```python
   > class Separator(object):
   >     def __init__(self, dict_path=DICT_PATH):
   >         pass
   >     
   >     # 使用词典分词的动态规划算法
   >     # content: 待分词语句
   >     def __dp(self, content):
   >         pass
   >     
   >     # 使用HMM模型的viterbi算法
   >     # content: 待分词语句
   >     def __viterbi(self, content):
   >         pass
   >     
   >     # 初始化词典
   >     # dict_file: 词典文件
   >     def init_dict(self, dict_file):
   >         pass
   >     
   >     # 初始化HMM模型
   >     def init_hmm(self):
   >         pass
   >     
   >     # 分词主函数
   >     # content: 分词内容，可以为长篇文章
   >     # dict_mode: 是否使用词典模式
   >     # 返回分词列表
   >     def cut(self, content, dict_mode=True):
   >         pass
   > ```
   >
   > Separator类提供切分方法cut，在其中将参数content按照标点符号和空格将待切分文本划分成若干句子，并根据dict_mode参数选择基于词典切分或基于HMM切分。
   >
   > * 基于词典的切分过程是先调用init_dict方法对词典进行初始化，然后调用__dp方法对中文句子进行切分
   > * 基于HMM的切分过程是先调用init_hmm方法对HMM模型进行初始化，然后调用__viterbi方法，寻找最优切分方案。
   >
   > 两种方法的初始化过程默认在Separator类的对象创建时就已经执行，对象创建后也可以选用自己的词典重新进行初始化。

   2.2 词性标注（tagger.py）

   > 将词性标注功能封装成词性标注器Tagger，主要基于一阶HMM模型实现，并加入Good-Turing平滑方法。其包含的主要成员方法如下：
   >
   > ```python
   > class Tagger(object):
   >     def __init__(self, dict_path=DICT_PATH, lib_path=CORPUS_PATH):
   >         pass
   >     
   >     # Good-Turing平滑算法
   >     # counts: 待平滑的列表
   >     @staticmethod
   >     def good_turing(counts):
   > 		pass
   >    	
   >     # Viterbi算法
   >     # ls: 分词后的词列表
   >     def __viterbi(self, ls):
   >      	pass
   >     
   >     # 使用语料库初始化
   >     # dict_file: 词典文件路径
   >     # corpus_file: 语料库文件路径
   >     def init_lib(self, dict_file, corpus_file):
   >         pass
   >     
   >     # 在词典中加入新词汇    
   >     # ls: 词汇列表
   >     def append_lib(self, ls):
   >         pass
   >     
   >     # 对已分词的列表进行词性标注
   >     # ls: 分词列表
   >     def tag_with_cut_list(self, ls):
   >         pass
   >     
   >     # 对句子进行分词和词性标注
   >     # content: 待分词内容
   >     # jieba_cut: 是否使用jieba进行分词
   >     # dict_mode: 是否使用Separator的词典模式进行分词
   >     # 返回值为二元组: (分词结果列表, 词性标注列表)
   >     def tag(self, content, jieba_cut=False, dict_mode=False, is_add_to_lib=True):
   >         pass
   >     
   > ```
   >
   > Tagger类提供词性标注函数tag和tag_with_cut_list，前者针对中文文本进行分词和词性标注，后者对已经切分好的词列表进行词性标注。
   >
   > 在Tagger类的对象被创建时进行词典和语料库的初始化，统计出一阶HMM模型中的发射矩阵、转移矩阵和初始状态矩阵，并调用good_turing方法对三个矩阵分别平滑处理。
   >
   > 调用tag方法时，首先对content进行分词，如果jieba_cut=True则使用jieba库进行分词，否则根据dict_mode选择使用基于词典的方法或者基于统计的方法进行分词，然后根据is_add_to_lib参数判断是否将分词列表加入到词典中。若有新词加入到矩阵中则需要对发射矩阵重新进行平滑处理。
   > 之后调用tag_with_cut_list方法对分好词的内容进行词性划分，调用__viterbi方法，使用Viterbi算法进行词性划分寻找最优标注方案。

   2.3 测试（test.py）

   > 测试程序包含两个部分：分词测试函数sep_test()和词性标注测试函数tag_test()，除此之外使用一个装饰器stat_time()来对两个函数的运行时间进行统计。
   >
   > 测试语料为《人民日报》6月份语料。
   >
   > * sep_test()
   >
   >   对Separator分词进行测试，并统计数据，计算召回率Recall、精确率Precision和F1值。默认将分词结果输出到sep.out.txt。
   >
   > * tag_test()
   >
   >   对Tagger词性标注进行测试，使用语料n中的分词结果进行词性标注，并计算词性标注的正确率。默认将分词结果输出到tag.out.txt。

3. 实现细节与算法原理

   3.1 基于词典的分词

   > 其主要原理是：首先对词典和词频进行统计，用词频/总词频作为词的概率，用每个词的前缀构造前缀字典。分词时，将文本根据前缀字典构造有向无环图（DAG），并在DAG上使用动态规划求解分词转移的最大概率，进而得到最优分词方案。
   >
   > * 前缀字典的构造过程：
   >
   > ```python
   > with open(dict_file, "r") as f:
   > 	for line in f:
   > 		word, freq = line.strip().split(' ')[0:2]
   >         self.trie[word] = int(freq)
   >         self.totf += int(freq)
   >         pw = u''
   >         for c in word:
   >         	pw += c
   >             if pw not in self.trie:
   >             	self.trie[pw] = 0
   > ```
   >
   > ``self.trie``统计了词典中每个词语的前缀及其出现的词频，``self.totf``用来统计总词频。对于词典中未出现的前缀，将其也加入到``self.trie``中，词频置为0。
   >
   > * DAG构造过程：
   >
   > ```python
   > n = len(content)
   > dag = [[] for i in range(n)]
   > for i in range(n):
   > 	pw = u''
   >     for j in range(i, n):
   >     	pw += content[j]
   >         if pw not in self.trie:
   >         	break
   >         if self.trie[pw] > 0:
   >         	dag[i].append(j + 1)
   >         if not dag[i]:
   >         	dag[i].append(i + 1)
   > ```
   >
   > ``dag[i]``存储从第i个字的所有出边。如果从content[i+1:j-1]的词语在前缀字典中存在且词频不为0，则从i到j加一条有向边。如果i没有出边，则从i到i+1加一条出边，以保证能够继续向下求解。
   >
   > * 动态规划过程：
   >
   > ```python
   > f = [(-inf, 0) for i in range(n + 1)]
   > f[0] = (0, 0)
   > for i in range(n):
   > 	for j in dag[i]:
   > 		val = f[i][0] + log(self.trie.get(content[i:j]) or 1) - log(self.totf)
   >         if f[j][0] < val:
   >         	f[j] = (val, i)
   > sep = []
   > i = n
   > while i != 0:
   > 	sep.append(i)
   >     i = f[i][1]
   > sep.reverse()
   > ```
   >
   > ``f``存储了动态规划转移的状态，``f[i]``为一个二元组：(content[1:i]分词的最大概率, 最大概率的转移路径)。则状态转移方程为：
   >
   > $f[i][0]=\max_\limits{0\leq j<i}\{f[j][0] \cdot \frac{trie[content[j:i]]}{totf}\}\\f[i][1]=\arg\max_\limits{0\leq j<i}\{f[j][0] \cdot \frac{trie[content[j:i]]}{totf}\}$
   >
   > 状态初始化为：$f[i][0]=0(1\leq i\leq n), f[0][0]=1$
   >
   > 但对于较长的句子，转移阶段过长，会导致$f[i][0]$的值十分接近于0。为了避免精度损失，使用a概率的对数运算代替概率运算：
   >
   > $f[i][0]=\max_\limits{0\leq j<i}\{f[j][0] + log(trie[content[j:i]]) - log(totf)\}\\f[i][1]=\arg\max_\limits{0\leq j<i}\{f[j][0] + log(trie[content[j:i]]) - log(totf)\}$
   >
   > 此时的状态初始化为：$f[i][0]=-inf,f[0][0]=0\ (-inf为一指定极小值常量)$
   >
   > 求得最优概率后，通过f中二元组存储的转移路径从后向前寻找分词位置即可。

   3.2 基于统计的分词

   > 主要使用了一阶HMM模型，将每个字的状态标记为{'B', 'M', 'E', 'S'}中的一种（B：词首、M：词中，E：词尾，S：单字），通过训练语料库来得到状态的转移概率矩阵、状态到词典的发射概率矩阵以及初始状态概率矩阵。
   >
   > * 在状态转移时，有些状态转移是可以省略的，如$B\rightarrow S$，因为词首的下一个状态不可能是单字。实际上每种状态均只有两种可能的转移状态，因此可以定义next_status来指定每种状态的可转移状态：
   >
   > ```python
   > self.next_status = {
   > 	'B': 'ME',
   >     'M': 'EM',
   >     'E': 'BS',
   >     'S': 'BS'
   > }
   > ```
   >
   > * Viterbi算法过程：
   >
   > ```python
   > n = len(content)
   > f = [{'B':(-inf, 'S'), 'M':(-inf, 'S'), 'E':(-inf, 'S'), 'S':(-inf, 'S')} for i in range(n)]
   > 
   > for i in self.status:
   > 	f[0][i] = (start_p[i] + emit_p[i].get(content[0], -inf), 'S')
   >         
   > for i in range(n - 1):
   > 	for j in self.status:
   >         for k in self.next_status[j]:
   >             val = f[i][j][0] + trans_p[j][k] + emit_p[k].get(content[i + 1], -inf)
   >             if f[i + 1][k][0] < val:
   >     	        f[i + 1][k] = (val, j)
   > ```
   >
   > ``f``存储了动态规划转移状态，``f[i][j]``是一个二元组：(当文本中第i个字状态为j时的最大概率, 最大概率的转移路径)。为了避免和基于词典进行分词时相同的概率精度问题，将状态转移矩阵trans_p、状态发射矩阵emit_p和初始状态矩阵start_p的值作取对数操作。因此状态转移方程为：
   >
   > $f[i][j][0]=\max_\limits{j\in next\_status[k]}f[i-1][k][0] + trans\_p[k][j]+emit\_p[j][content[i]]\\f[i][j][1]=\arg\max_\limits{j\in next\_status[k]}f[i-1][k][0] + trans\_p[k][j]+emit\_p[j][content[i]]$
   >
   > 初始化状态：
   >
   > $f[i][j][0]=-inf(1\leq i\leq n)\\f[0][j][0]=start\_p[j]+emit\_p[j][content[0]]$
   >
   > 求得最优概率后，同样根据f的二元组中的最大概率的转移路径寻找每个字的状态，根据状态进行分词即可：
   >
   > ```python
   > # result为viterbi算法求得的每个字的状态标记
   > blocks = []
   > result = self.__viterbi(sentence)
   > lst = 0
   > for i in range(len(sentence)):
   > 	if result[i] in 'SE':
   >     	blocks.append(sentence[lst:i + 1])
   >         lst = i + 1
   > ```
   >
   > 

   3.3 基于一阶HMM模型的词性标注

   > HMM模型包含三个部分：初始概率分布、状态转移概率分布和观测概率分布。对于词性标注任务，初始概率分布即对于词性i，其出现在句首的概率；状态转移概率分布即对于两个词性i和j，i的下一个词性是j的概率；观测概率分布即对于词性i和词w，当词性为i时词为w的概率。
   >
   > 一阶HMM模型主要基于两点假设：
   >
   > * 马尔可夫性假设，即隐马尔可夫链在任意时刻的状态只依赖于前一时刻的状态：
   >
   > $$
   > p(i_t|i_{t-1}o_{t-1}...i_1o_1)=p(i_t|i_{t-1})
   > $$
   >
   > * 观测独立性假设，即任意时刻的观测只依赖于该时刻的马尔可夫链的状态：
   >
   > $$
   > p(o_t|i_To_Ti_{T-1}o_{T-1}...i_to_t...i_1o_1)=p(o_t|i_t)
   > $$
   >
   > 因此最终目标有如下转化：
   > $$
   > p(i_Ti_{T-1}...i_1|o_To_{T-1}...o_1)=p(i_T|i_{T-1})p(o_T|i_T)\cdot p(i_{T-1}|i_{T-2})p(o_{T-1}|i_{T-1})\cdot...\cdot p(i_2|i_1)p(o_2|i_2)\cdot p(i_1|\$)p(o_1|i_1)
   > $$
   > 其中$i_t$代表第t个词的词性，$o_t$代表第t个词的内容，$\$$代表句首。
   >
   > 基于一阶HMM模型分词基本过程是使用《人民日报》语料库统计出发射矩阵、转移矩阵和初始状态矩阵，并根据上述原理，采用Viterbi算法求解对文本词性标注的最大概率。
   >
   > * 使用语料库统计过程代码：
   >
   > ```python
   > with open(corpus_file, 'r', encoding='utf-8') as f:
   > 	for line in f.readlines():
   >     	ls = line.strip().split('  ')
   >         lst = None
   >         for ch in ls:
   >             wd, tp = ch.split('/')[0:2]
   >             if self.wid.get(wd) is None:
   >             	self.wtot += 1
   >                 self.wid[wd] = self.wtot
   >                 for i in self.tid.values():
   >                 	self.w_cnt[i].append(0)
   >             wd = self.wid[wd]
   >             tp = self.tid[tp.split(']')[0]]
   >                     
   >             self.t_cnt[tp] += 1
   >             self.w_cnt[tp][wd] += 1
   >             if lst is not None:
   >             	self.mat[lst][tp] += 1
   >             lst = tp
   > ```
   >
   > 其中``w_cnt, t_cnt, mat``分别为观测频度、初始状态频度、状态转移频度。统计完成后再进一步进行归一化处理。
   >
   > * Viterbi算法词性标注代码：
   >
   > ```python
   > 		# dp: 动态规划的状态转移数组
   >         dp = [[-inf for i in range(self.tsz)] for j in range(n + 10)]
   >         # path: 动态规划的状态转移路径数组
   >         path = [[0 for i in range(self.tsz)] for j in range(n + 10)]
   >         # 初始化dp数组 
   >         for tp in self.tps:
   >             i = self.tid[tp]
   >             dp[0][i] = (log(self.t_cnt[i]) if self.t_cnt[i] != 0 else -inf) + \
   >                 (log(self.w_prob[i][self.wid[ls[0]]]) if self.w_prob[i][self.wid[ls[0]]] != 0 else -inf)
   >         
   > 
   >         # Viterbi
   >         for i in range(n - 1):
   >             w = ls[i]
   >             w_nxt = ls[i + 1]
   >             for tp in self.tps:
   >                 j = self.tid[tp]
   >                 wj = self.wid[w]
   >                 for tp_nxt in self.tps:
   >                     k = self.tid[tp_nxt]
   >                     wk = self.wid[w_nxt]
   >                     p = dp[i][j] + \
   >                         (log(self.mat[j][k]) if self.mat[j][k] != 0 else -inf) + \
   >                         (log(self.w_prob[k][wk]) if self.w_prob[k][wk] != 0 else -inf)
   >                     if p > dp[i + 1][k]:
   >                         dp[i + 1][k] = p
   >                         path[i + 1][k] = j
   > ```
   >
   > 其中``w_prob``是统计的观测频度进行平滑和归一化后的发射矩阵，``tps``是词性集合，``tsz``是词性集合的大小。为了避免精度问题，这里同样对所有概率作取对数操作。
   >
   > ``dp[i][j]``代表转移到第i个词、该词的词性为j时的最大概率，``path``用来记录动态规划状态转移时的最优路径
   >
   > 状态转移方程如下：
   > $$
   > dp[i][j]=\max_\limits{k}\{dp[i-1][k]+\log(mat[k][j])+\log(w\_prob[j][w_i])\}\\
   > path[i][j]=\arg\max_\limits{k}\{dp[i-1][k]+\log(mat[k][j])+\log(w\_prob[j][w_i])\}
   > $$
   > 初始化状态：
   > $$
   > dp[i][j]=-inf\\
   > dp[0][j]=\log(t\_cnt[j])+\log(w\_prob[j][w_0])
   > $$
   > 得到最优概率``dp[n][j]``后，即可根据``path``记录的最优路径从后向前寻找每个词的词性。

   3.4 数据平滑处理

   > 在进行词性标注时，由于出现很多未登录词，因此出现许多发射概率为0的情况。为了解决这种数据稀疏的问题，采用Good Turing平滑算法对统计结果进行平滑。Good Turing的基本思路是利用频率的类别信息来平滑频率。对于发生了$r$次的$n$元语法，假设它发生了$r^*$次：
   > $$
   > r^*=(r+1)\frac{n_{r+1}}{n_r}
   > $$
   > 其中$n_r$代表正好发生了$r$次的$N$元组的个数。
   >
   > * Good Turing平滑算法代码如下：
   >
   > ```python
   > @staticmethod
   >     def good_turing(counts):
   >         N = sum(counts)
   >         # 存储平滑后的值
   >         prob = [0] * len(counts)
   >     
   >         if N == 0:
   >             return prob
   >         
   >         Nr = [0] * (max(counts) + 1)
   >         for r in counts:
   >             Nr[r] += 1
   >         # 平滑范围，只对r小于8的值进行平滑
   >         smooth_boundary = min(len(Nr)-1, 8)  
   > 
   >         # 平滑过程
   >         for r in range(smooth_boundary):
   >             if Nr[r] != 0 and Nr[r+1] != 0:
   >                 Nr[r] = (r+1) * Nr[r+1] / Nr[r]
   >             else:
   >                 Nr[r] = r
   >         for r in range(smooth_boundary, len(Nr)):
   >             Nr[r] = r
   >        
   >         for i in range(len(counts)):
   >             prob[i] = Nr[counts[i]]
   >         total = sum(prob)
   > 
   >         # 归一化
   >         return [p/total for p in prob]
   > ```
   >
   > * 对发射矩阵、转移矩阵和初始状态矩阵均进行平滑处理：
   >
   > ```python
   > for t in range(self.tsz):
   >     # 发射矩阵
   > 	self.w_prob[t] = self.good_turing(self.w_cnt[t]) 
   >     # 转移矩阵
   >     self.mat[t] = self.good_turing(self.mat[t])
   > #初始状态矩阵
   > self.t_cnt = self.good_turing(self.t_cnt)
   > ```

4. 实现过程中遇到的问题

   * 动态规划过程中，状态转移的概率很接近0，造成精度损失，出现大量概率为0的中间状态

     > 解决方案：将概率的乘法转化为概率对数的加法：
     >
     > $p_i\cdot p_j\rightarrow \log(p_i) + log(p_j)$
     >
     > 对于$p_i=0$的情况，近似取$\log(p_i)=-\inf$
   
   * 词性标注时出现大量未登录词，导致大量观测概率为0，词性标注效果较差

	  >解决方案：使用Good Turing平滑算法对发射矩阵进行平滑处理。为了增加结果准确性，仅      对$r\lt 8$的n元语法进行平滑处理。
      >
	  >对于待标注文本，如果在动态规划之前将文本分词结果直接加入到发射矩阵，则每次都需要      重新对发射矩阵进行平滑，导致效率下降。因此在词性标注前直接将整篇文本进行分词并加      入发射矩阵，这时只需进行一次平滑即可。

   * python在代码中无法导入同目录下的另一个源代码
   
     > 解决方案：添加\_\_init\_\_.py和\_\_main__.py文件，将整个目录视为一个模块，在运行时使用python -m参数调用模块内的源程序。
   
   * 基于词典的分词对未登录词无法识别的问题
   
     > 解决方案：先对文本使用基于词典的分词方法，对于未登录词再使用基于HMM的分词方法，对未登录词的分词正确率会大大上升。